{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROFT-Prompt-Sampler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgZmTYmKfNtw"
      },
      "source": [
        "# ROFT Prompt Sampler\n",
        "This notebook efficiently samples prompts from large corpora for use in the RoFT project http://roft.io\n",
        "\n",
        "**Input:** file of articles or stories -- one full article per line\n",
        "\n",
        "**Output:** \"prompts-{sample filename}.json\" in format:\n",
        "  ```\n",
        "{\n",
        "  \"dataset\": \"nyt-articles\",\n",
        "  \"split\": \"dev\",\n",
        "  \"date-sampled\": \"17/09/2020\",\n",
        "  \"prompts\": [\n",
        "    [\n",
        "      \"The Newark Teachers Union filed a follow-up complaint with the Federal Department of Education yesterday, accusing the Newark school district of continuing to violate Federal laws barring discrimination against handicapped students.\",\n",
        "      \"Under an agreement brokered in January, officials at the state-run district agreed to a 15-point plan intended to bring Newark schools into compliance with Federal laws.\",\n",
        "      \"That plan has still not been implemented, said Mitchell Gerry, a spokesman for the 5,000-member union, which sent the complaint letter to the department's Office of Civil Rights.\",\n",
        "      \"E. J. Miranda, a spokesman for the district, said the district had been making efforts to comply with the Rehabilitation Act of 1973.\"\n",
        "    ],\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6VvE4CvmuHH"
      },
      "source": [
        "# Usage\n",
        "\n",
        "Just edit the three cells below with your desired preferences and run!\n",
        "\n",
        "# Notes\n",
        "1. PERCENT_MAX is an upper bound for the percentage of max length prompts in the output. If the sampler attempts to sample at max length and the article is shorter than max it will accept the full length of that article instead of resampling for another longer article. You can check how many of your samples had this happen by checking the warning at the end of the sampling prints \"Warning: 1 articles were too short for prompt length\" This may be something worth fixing in the future. \n",
        "\n",
        "2. The sampler assumes the file extension to a sample file is .txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFX2fhTqqbyq"
      },
      "source": [
        "''' Browse the cloud bucket for your desired sample file '''\n",
        "!gsutil ls gs://roft_datasets/data/story_prompts/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AP2CNY9aVT3"
      },
      "source": [
        "''' Once you've found it, download it '''\n",
        "!gsutil cp gs://roft_datasets/data/speeches/test.txt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ESi-V_nbBt"
      },
      "source": [
        "''' Set your preferences and you're good to go! '''\n",
        "SAMPLE_FILE_PATH = '/content/ai_dungeon-train.txt' # The path to the downloaded sample file\n",
        "NUM_SAMPLES = 500000 # The total number of prompts you want to sample pre-filter (will default to total number of articles in input file when too large)\n",
        "MAX_LEN = 10 # The maximum number of sentences of each prompt\n",
        "PERCENT_MAX = 1.0 # The percentage of prompts that will be sampled at that maximum length (i.e. the percentage of \"all human\" examples)\n",
        "BOW_FILTER_TOGGLE = True # Filter duplicate sentences with bag of words method\n",
        "VERB_FILTER_TOGGLE = True # Filter out any prompt that contains a sentence without a verb\n",
        "REJECT_TOO_SHORT = True # Reject the prompt when it is shorter than the desired sampling length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCKc3i53aDnD"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import mmap\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import date\n",
        "from spacy.lang.en import English\n",
        "from spacy.pipeline import Sentencizer\n",
        "\n",
        "nlp = English()\n",
        "sentencizer = Sentencizer()\n",
        "nlp.add_pipe(sentencizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2UVCN5MZ6bo"
      },
      "source": [
        "class PromptSampler:\n",
        "  def print_prompt_sampled(self, prompt, index, total, line):\n",
        "    print('Sampled prompt {0}/{1} of length {2} from line {3}'.format(\n",
        "            str(index), str(total), str(len(prompt)), str(line)))\n",
        "    for line in prompt:\n",
        "      print('\\t' + repr(line))\n",
        "\n",
        "  def print_prompt_too_short_warning(self, index, article_len, prompt_len):\n",
        "    print('Warning: Article #{0} (len: {1}) is too short for prompt length of {2}'.format(\n",
        "              str(index), str(article_len), str(prompt_len)))\n",
        "    \n",
        "  def random_sample_prompt_len(self, percent_human, max_prompt_len):\n",
        "    if (random.random() < percent_human):\n",
        "      return max_prompt_len\n",
        "    else:\n",
        "      return random.randint(1, max_prompt_len)\n",
        "      \n",
        "  def sample_corpus(self, sample_file, num_samples, max_prompt_len, percent_human, random_seed=436421):\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    if not os.path.exists(sample_file):\n",
        "      print('Error: sample file \"' + sample_file + '\" does not exist')\n",
        "      exit(-1)\n",
        "\n",
        "    prompts = [] # The 2D array of prompts\n",
        "    num_shortened = 0 # The number of prompts that were too small to be full length\n",
        "    with open(sample_file, 'r+b') as f:\n",
        "      # mmap the file to avoid loading the whole thing into RAM\n",
        "      map_file = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)\n",
        "      \n",
        "      # Randomly decide which articles to grab our prompts from\n",
        "      wc_output = !wc -l $sample_file\n",
        "      num_lines = int(wc_output[0].split()[0])\n",
        "\n",
        "      # If we ask for more samples than we can give, give as much as we can\n",
        "      if num_samples > num_lines: num_samples = num_lines\n",
        "      \n",
        "      articles_to_sample = random.sample(range(num_lines), num_samples)\n",
        "\n",
        "      # Iterate over all articles in the file and sample from only the selected articles\n",
        "      for index, line in enumerate(iter(map_file.readline, b\"\")):\n",
        "        if index not in articles_to_sample: continue\n",
        "\n",
        "        # Randomly determine this prompt's length based on the percent human value\n",
        "        prompt_length = self.random_sample_prompt_len(percent_human, max_prompt_len)\n",
        "\n",
        "        # Use Spacy sentence tokenizer to split this prompt into sentences\n",
        "        article = list(nlp(str(line, 'utf-8', 'ignore')).sents)\n",
        "\n",
        "        # Strip leading and trailing whitespace and filter out empty lines\n",
        "        article = [str(sent).strip() for sent in article if len(str(sent).strip()) > 0]\n",
        "\n",
        "        # If article is shorter than the desired prompt length, shorten the prompt length\n",
        "        if len(article) < prompt_length:\n",
        "          num_shortened += 1\n",
        "          if REJECT_TOO_SHORT: continue \n",
        "          self.print_prompt_too_short_warning(index, len(article), prompt_length)\n",
        "          prompt_length = len(article)\n",
        "\n",
        "        # Append the prompt to the list of prompts\n",
        "        prompts.append(article[:prompt_length])\n",
        "\n",
        "        self.print_prompt_sampled(article[:prompt_length], len(prompts), num_samples, index)\n",
        "    \n",
        "    print('Warning: {0} articles were too short for prompt length'.format(str(num_shortened)))\n",
        "    return prompts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voVdGPs7gEPi"
      },
      "source": [
        "# Sample the prompts\n",
        "sampler = PromptSampler()\n",
        "prompts = sampler.sample_corpus(SAMPLE_FILE_PATH, NUM_SAMPLES, MAX_LEN, PERCENT_MAX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNuXm5hrZJCa"
      },
      "source": [
        "if BOW_FILTER_TOGGLE:\n",
        "  # Naively filter prompts based on bag of words similarity\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  THRESHOLD = 1\n",
        "\n",
        "  for index, prompt in enumerate(prompts):\n",
        "    take_out_prompt = False\n",
        "    tokenized_prompt = [list(tokenizer(str(sentence))) for sentence in prompt]\n",
        "  \n",
        "    # Cast all to string\n",
        "    for i, sentence in enumerate(tokenized_prompt):\n",
        "      for j, word in enumerate(sentence):\n",
        "        tokenized_prompt[i][j] = str(word)\n",
        "\n",
        "    # Look for duplicates naively\n",
        "    for i, curr_sent in enumerate(tokenized_prompt):\n",
        "      for j, target_sent in enumerate(tokenized_prompt):\n",
        "        if len(curr_sent) < THRESHOLD * 2 or len(target_sent) < THRESHOLD * 2: \n",
        "          continue\n",
        "        if i == j: continue\n",
        "        distinct_words = 0\n",
        "        for word in curr_sent:\n",
        "          if word not in target_sent:\n",
        "            distinct_words += 1\n",
        "          if distinct_words > THRESHOLD:\n",
        "            break\n",
        "        if distinct_words > THRESHOLD:\n",
        "            break\n",
        "        else:\n",
        "          print('---------------------------------------------------')\n",
        "          print(' '.join(curr_sent))\n",
        "          print(\" too similar to \") \n",
        "          print(' '.join(target_sent))\n",
        "          take_out_prompt = True\n",
        "          break\n",
        "      if take_out_prompt:\n",
        "        break\n",
        "    if take_out_prompt:    \n",
        "      prompts[index] = [[\"REMOVED\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5XCfPaqhDKM"
      },
      "source": [
        "# FIlter based on presence of verb in the sentence\n",
        "import spacy\n",
        "\n",
        "if VERB_FILTER_TOGGLE:\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  for index, prompt in enumerate(prompts):\n",
        "    remove_prompt = False\n",
        "    for sentence in prompt:\n",
        "      doc = nlp(str(sentence))\n",
        "      pos = [token.pos_ for token in doc]\n",
        "      if len(pos) == 1 and pos[0] == \"SPACE\":\n",
        "        continue\n",
        "      if \"VERB\" not in pos and \"AUX\" not in pos:\n",
        "        print(\"No verbs in sentence: \" + str(sentence))\n",
        "        print(\"POS Tags: \" + str([token.pos_ for token in doc]))\n",
        "        remove_prompt = True\n",
        "        break\n",
        "    if remove_prompt:\n",
        "      prompts[index] = [[\"REMOVED\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h732qobwlz1R"
      },
      "source": [
        "filtered_prompts = [prompt for prompt in prompts if prompt[0][0] is not 'REMOVED']\n",
        "cast_prompts = [[str(sent) for sent in prompt] for prompt in filtered_prompts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcHvn8sNpBLb"
      },
      "source": [
        "len(filtered_prompts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jogg03bb3JMI"
      },
      "source": [
        "# Save the prompts to the json file\n",
        "sample_file_name = SAMPLE_FILE_PATH.split('/')[-1]\n",
        "to_save = {\n",
        "  'sample-file': sample_file_name,\n",
        "  'date-sampled': date.today().strftime(\"%d/%m/%Y\"),\n",
        "  'prompts': cast_prompts\n",
        "}\n",
        "  \n",
        "with open('prompts-{}.json'.format(sample_file_name[:-4]), 'w') as f:\n",
        "  json.dump(to_save, f, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}